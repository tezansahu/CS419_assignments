{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader(object):\n",
    "    # this class has a standard iterator declared\n",
    "    # __len__ returns the number of batches (size of the object)\n",
    "    # __get_item__ handles integer based indexing of the object \n",
    "    def __init__(self, data_file, batch_size):\n",
    "        with open(data_file, 'r') as df:\n",
    "            data = df.readlines()\n",
    "\n",
    "        data = data[1:]\n",
    "        data = data[:(len(data)//batch_size)*batch_size]\n",
    "        np.random.shuffle(data)\n",
    "        data = np.array([[float(col) for col in row.split(',')] for row in data])\n",
    "        input_data, targets = data[:, :-1], data[:, -1]\n",
    "        input_data = np.hstack([input_data, np.ones((len(input_data), 1), dtype=np.float32)])\n",
    "\n",
    "        self.num_features = input_data.shape[1]\n",
    "        self.current_batch_index = 0\n",
    "        self.input_batches = np.split(input_data, len(input_data)//batch_size)\n",
    "        self.target_batches = np.split(targets, len(targets)//batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_batches)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "\n",
    "        batch_input_data = self.input_batches[i]\n",
    "        batch_targets = self.target_batches[i]\n",
    "        return batch_input_data, batch_targets\n",
    "\n",
    "def classify(inputs, weights):\n",
    "    #this functions returns w^Tx . The output  is batch_size*1\n",
    "\treturn np.dot(inputs, np.reshape(weights, (np.size(weights), 1)).reshape((-1,)))\n",
    "\n",
    "def get_objective_function(trainx,trainy,loss_type, regularizer_type, loss_weight):\n",
    "    # this function calculates the loss for a current batch\n",
    "    loss_function = utils.loss_functions[loss_type]\n",
    "    if regularizer_type != None:\n",
    "\n",
    "        regularizer_function = utils.regularizer_functions[regularizer_type]\n",
    "    def objective_function(weights):\n",
    "        loss = 0\n",
    "        \n",
    "        inputs, targets = trainx,trainy\n",
    "        outputs = classify(inputs, weights)\n",
    "        loss += loss_weight*loss_function(targets, outputs)\n",
    "        if regularizer_type != None:\n",
    "            # regulariser function is called from utils.py\n",
    "            loss += regularizer_function(weights)\n",
    "        return loss\n",
    "    return objective_function\n",
    "\n",
    "def get_gradient_function(trainx,trainy,loss_type, regularizer_type, loss_weight):\n",
    "    # This is a way to declare function inside a function \n",
    "    # The get_gradient_function receives the train data from the current batch\n",
    "    # and all other parameters on which the loss function and gradient depend\n",
    "    # like C,regulariser_type and loss function\n",
    "    loss_grad_function = utils.loss_grad_functions[loss_type]\n",
    "    if regularizer_type != None:\n",
    "        regularizer_grad_function = utils.regularizer_grad_functions[regularizer_type]\n",
    "    # gradient function is called from scipy.optimise.minimise()\n",
    "    # the only paramter its can send is weights \n",
    "    # hence there was a need to pass the current batch through get_objective_function\n",
    "\n",
    "\n",
    "    def gradient_function(weights):\n",
    "\n",
    "        gradient = np.zeros(len(weights), dtype=np.float32)\n",
    "        X=trainx\n",
    "        Y=trainy\n",
    "        outputs = classify(X,weights)\n",
    "        # loss_grad_function is called from utils.py\n",
    "        gradient = loss_weight*loss_grad_function(weights,X,Y,outputs)/len(trainx)\n",
    "        if regularizer_type != None:\n",
    "            # regulariser grad function is called from utils.py\n",
    "            gradient += regularizer_grad_function(weights)\n",
    "        return gradient\n",
    "    return gradient_function\n",
    "\n",
    "def train(data_loader, loss_type, regularizer_type, loss_weight):\n",
    "    initial_model_parameters = np.random.random((data_loader.num_features))\n",
    "\n",
    "    num_epochs=1000\n",
    "    for i in range(num_epochs):\n",
    "        loss=0\n",
    "        if(i==0):\n",
    "            start_parameters=initial_model_parameters\n",
    "        for j in range(len(data_loader)):\n",
    "            trainx,trainy=data_loader[j]\n",
    "            objective_function = get_objective_function(trainx,trainy,loss_type, \n",
    "                                                regularizer_type,loss_weight)\n",
    "            gradient_function = get_gradient_function(trainx,trainy, loss_type, \n",
    "                                              regularizer_type, loss_weight)\n",
    "            # to know about this function please read about scipy.optimise.minimise\n",
    "            trained_model_parameters = minimize(objective_function, \n",
    "                                        start_parameters, \n",
    "                                        method=\"CG\", \n",
    "                                        jac=gradient_function,\n",
    "                                        options={'disp': False,\n",
    "                                                 'maxiter': 1})\n",
    "            loss+=objective_function(trained_model_parameters.x)\n",
    "            start_parameters=trained_model_parameters.x\n",
    "        # prints the batch loss\n",
    "        print(\"loss is  \",loss)\n",
    "        \n",
    "    print(\"Optimizer information:\")\n",
    "    print(trained_model_parameters)\n",
    "    return trained_model_parameters.x\n",
    "            \n",
    "\n",
    "def test(inputs, weights):\n",
    "    outputs = classify(inputs, weights)\n",
    "    probs = 1/(1+np.exp(-outputs))\n",
    "    # this is done to get all terms in 0 or 1 You can change for -1 and 1\n",
    "    return np.round(probs)\n",
    "\n",
    "def write_csv_file(outputs, output_file):\n",
    "    # dumps the output file\n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        out_file.write(\"ID, Output\\n\")\n",
    "        for i in range(len(outputs)):\n",
    "            out_file.write(\"{}, {}\".format(i+1, str(outputs[i])) + \"\\n\")\n",
    "def get_data(data_file):\n",
    "    with open(data_file, 'r') as df:\n",
    "        data = df.readlines()\n",
    "\n",
    "    data = data[1:]\n",
    "    data = np.array([[float(col) for col in row.split(',')] for row in data])\n",
    "    input_data = np.hstack([data, np.ones((len(data), 1), dtype=np.float32)])\n",
    "\n",
    "    return input_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got files\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = DataLoader(\"train.csv\", 64)\n",
    "test_data = get_data(\"test.csv\")\n",
    "print(\"Got files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "actual_targets=pd.read_csv(\"targets.csv\")\n",
    "actual_targets=actual_targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(actual_targets[:,1]==test_data_output)/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training for c =  100\n",
      "loss is   60549.361006675055\n",
      "loss is   48699.356154363886\n",
      "loss is   43305.12098788081\n",
      "loss is   40143.47050626809\n",
      "loss is   38050.62114844968\n",
      "loss is   36413.665967636814\n",
      "loss is   35086.71720576822\n",
      "loss is   33986.1041987825\n",
      "loss is   33064.181237251716\n",
      "loss is   32258.597455616018\n",
      "loss is   31603.28441350245\n",
      "loss is   31005.128144387774\n",
      "loss is   30544.924276204114\n",
      "loss is   30112.64203267228\n",
      "loss is   30265.81583500657\n",
      "loss is   29436.117789910473\n",
      "loss is   28996.33621456001\n",
      "loss is   28745.10642283185\n",
      "loss is   29087.546076535487\n",
      "loss is   28953.593201528114\n",
      "loss is   28851.382251260555\n",
      "loss is   28714.584174439475\n",
      "loss is   28593.70688808258\n",
      "loss is   28485.892355620857\n",
      "loss is   28386.948773055694\n",
      "loss is   28300.290186224935\n",
      "loss is   28222.808634662255\n",
      "loss is   28153.481415698716\n",
      "loss is   28091.416451758596\n",
      "loss is   28011.716076918063\n",
      "loss is   27984.87638401128\n",
      "loss is   27932.351023005245\n",
      "loss is   27886.041611359266\n",
      "loss is   27848.777532158012\n",
      "loss is   27817.42577711142\n",
      "loss is   27790.189373115772\n",
      "loss is   27765.96622559737\n",
      "loss is   27744.325825799217\n",
      "loss is   27724.91792735622\n",
      "loss is   27707.468199775347\n",
      "loss is   27691.749939057285\n",
      "loss is   27677.572595022517\n",
      "loss is   27664.77444325011\n",
      "loss is   27653.2115691921\n",
      "loss is   27642.75945667188\n",
      "loss is   27633.307776650734\n",
      "loss is   27624.758314356368\n",
      "loss is   27617.024049421423\n",
      "loss is   27610.026131403632\n",
      "loss is   27603.69255025402\n",
      "loss is   27597.9599337483\n",
      "loss is   27592.77061488761\n",
      "loss is   27588.072682417762\n",
      "loss is   27583.81958795588\n",
      "loss is   27579.9687635694\n",
      "loss is   27576.48200692276\n",
      "loss is   27573.32477163806\n",
      "loss is   27570.47729826085\n",
      "loss is   27567.900276224835\n",
      "loss is   27565.56592440547\n",
      "loss is   27563.451243690273\n",
      "loss is   27561.535446614977\n",
      "loss is   27559.799744283024\n",
      "loss is   27558.227339530786\n",
      "loss is   27556.801807771822\n",
      "loss is   27555.508277836125\n",
      "loss is   27554.335199293364\n",
      "loss is   27553.27166773648\n",
      "loss is   27552.307601665798\n",
      "loss is   27551.433768782525\n",
      "loss is   27550.64175552045\n",
      "loss is   27549.923914163945\n",
      "loss is   27549.27330283394\n",
      "loss is   27548.683625174057\n",
      "loss is   27548.149172730366\n",
      "loss is   27547.664771238702\n",
      "loss is   27547.2257311924\n",
      "loss is   27546.827802666652\n",
      "loss is   27546.46713419603\n",
      "loss is   27546.140235423678\n",
      "loss is   27545.843943219148\n",
      "loss is   27545.575390964514\n",
      "loss is   27545.331980721417\n",
      "loss is   27545.111358014445\n",
      "loss is   27544.911388985834\n",
      "loss is   27544.73013969858\n",
      "loss is   27544.565857385838\n",
      "loss is   27544.416953462845\n",
      "loss is   27544.28198813481\n",
      "loss is   27544.159656450436\n",
      "loss is   27544.04877566506\n",
      "loss is   27543.948273789087\n",
      "loss is   27543.857179211693\n",
      "loss is   27543.77461129688\n",
      "loss is   27543.6997718619\n",
      "loss is   27543.63193745437\n",
      "loss is   27543.570452353422\n",
      "loss is   27543.514722226366\n",
      "loss is   27543.464208380447\n",
      "loss is   27543.418422552444\n",
      "loss is   27543.376922186857\n",
      "loss is   27543.33930615628\n",
      "loss is   27543.30521088265\n",
      "loss is   27543.274306821542\n",
      "loss is   27543.246295276113\n",
      "loss is   27543.220905509148\n",
      "loss is   27543.19789212557\n",
      "loss is   27543.177032699976\n",
      "loss is   27543.158125626716\n",
      "loss is   27543.14098817048\n",
      "loss is   27543.125454700355\n",
      "loss is   27543.111375088327\n",
      "loss is   27543.098613258415\n",
      "loss is   27543.087045871172\n",
      "loss is   27543.076561131496\n",
      "loss is   27543.067057708144\n",
      "loss is   27543.05844375422\n",
      "loss is   27543.050636019478\n",
      "loss is   27543.043559045607\n",
      "loss is   27543.037144436872\n",
      "loss is   27543.031330199286\n",
      "loss is   27543.026060141146\n",
      "loss is   27543.021283330145\n",
      "loss is   27543.016953600894\n",
      "loss is   27543.013029109006\n",
      "loss is   27543.009471926467\n",
      "loss is   27543.00624767521\n",
      "loss is   27543.003325194648\n",
      "loss is   27543.000676240823\n",
      "loss is   27542.99827521321\n",
      "loss is   27542.996098907246\n",
      "loss is   27542.994126290465\n",
      "loss is   27542.992338298573\n",
      "loss is   27542.9907176519\n",
      "loss is   27542.989248687867\n",
      "loss is   27542.987917209903\n",
      "loss is   27542.986710350124\n",
      "loss is   27542.985616445003\n",
      "loss is   27542.984624922687\n",
      "loss is   27542.983726200746\n",
      "loss is   27542.98291159365\n",
      "loss is   27542.982173228764\n",
      "loss is   27542.981503970273\n",
      "loss is   27542.980897350233\n",
      "loss is   27542.98034750606\n",
      "loss is   27542.979849123923\n",
      "loss is   27542.97939738724\n",
      "loss is   27542.978987930328\n",
      "loss is   27542.97861679603\n",
      "loss is   27542.978280397627\n",
      "loss is   27542.977975483973\n",
      "loss is   27542.97769910836\n",
      "loss is   27542.9774485998\n",
      "loss is   27542.977221537283\n",
      "loss is   27542.977015726396\n",
      "loss is   27542.976829178097\n",
      "loss is   27542.976660089604\n",
      "loss is   27542.976506826722\n",
      "loss is   27542.976367908308\n",
      "loss is   27542.976241991775\n",
      "loss is   27542.97612786024\n",
      "loss is   27542.976024410713\n",
      "loss is   27542.97593064341\n",
      "loss is   27542.975845652152\n",
      "loss is   27542.975768615528\n",
      "loss is   27542.975698789076\n",
      "loss is   27542.975635497904\n",
      "loss is   27542.97557813042\n",
      "loss is   27542.975526132162\n",
      "loss is   27542.97547900064\n",
      "loss is   27542.97543628033\n",
      "loss is   27542.975397558355\n",
      "loss is   27542.97536246056\n",
      "loss is   27542.975330647652\n",
      "loss is   27542.975301812236\n",
      "loss is   27542.97527567566\n",
      "loss is   27542.975251985285\n",
      "loss is   27542.975230512184\n",
      "loss is   27542.975211048848\n",
      "loss is   27542.975193407143\n",
      "loss is   27542.975177416563\n",
      "loss is   27542.97516292264\n",
      "loss is   27542.97514978527\n",
      "loss is   27542.97513787747\n",
      "loss is   27542.975127084155\n",
      "loss is   27542.975117300997\n",
      "loss is   27542.975108433522\n",
      "loss is   27542.975100395935\n",
      "loss is   27542.97509311067\n",
      "loss is   27542.975086507286\n",
      "loss is   27542.975080521886\n",
      "loss is   27542.975075096674\n",
      "loss is   27542.975070179218\n",
      "loss is   27542.975065722057\n",
      "loss is   27542.975061682024\n",
      "loss is   27542.97505802013\n",
      "loss is   27542.97505470098\n",
      "loss is   27542.975051692418\n",
      "loss is   27542.97504896551\n",
      "loss is   27542.97504649379\n",
      "loss is   27542.97504425341\n",
      "loss is   27542.975042222737\n",
      "loss is   27542.975040382073\n",
      "loss is   27542.975038713736\n",
      "loss is   27542.975037201548\n",
      "loss is   27542.975035830867\n",
      "loss is   27542.97503458847\n",
      "loss is   27542.975033462353\n",
      "loss is   27542.97503244161\n",
      "loss is   27542.975031516464\n",
      "loss is   27542.97503067782\n",
      "loss is   27542.97502991777\n",
      "loss is   27542.975029228775\n",
      "loss is   27542.975028604316\n",
      "loss is   27542.975028038254\n",
      "loss is   27542.975027525204\n",
      "loss is   27542.975027060194\n",
      "loss is   27542.97502663867\n",
      "loss is   27542.975026256612\n",
      "loss is   27542.975025910324\n",
      "loss is   27542.975025596406\n",
      "loss is   27542.975025311905\n",
      "loss is   27542.97502505405\n",
      "loss is   27542.975024820254\n",
      "loss is   27542.975024608393\n",
      "loss is   27542.975024416362\n",
      "loss is   27542.97502424231\n",
      "loss is   27542.97502408449\n",
      "loss is   27542.975023941504\n",
      "loss is   27542.975023811887\n",
      "loss is   27542.975023694402\n",
      "loss is   27542.975023587886\n",
      "loss is   27542.97502349139\n",
      "loss is   27542.975023403884\n",
      "loss is   27542.975023324587\n",
      "loss is   27542.975023252704\n",
      "loss is   27542.975023187548\n",
      "loss is   27542.975023128456\n",
      "loss is   27542.97502307495\n",
      "loss is   27542.975023026447\n",
      "loss is   27542.975022982446\n",
      "loss is   27542.975022942584\n",
      "loss is   27542.975022906452\n",
      "loss is   27542.975022873696\n",
      "loss is   27542.975022844024\n",
      "loss is   27542.975022817136\n",
      "loss is   27542.975022792703\n",
      "loss is   27542.975022770617\n",
      "loss is   27542.97502275058\n",
      "loss is   27542.97502273242\n",
      "loss is   27542.97502271596\n",
      "loss is   27542.975022701074\n",
      "loss is   27542.975022687508\n",
      "loss is   27542.97502267524\n",
      "loss is   27542.97502266416\n",
      "loss is   27542.975022654064\n",
      "loss is   27542.975022644947\n",
      "loss is   27542.975022636663\n",
      "loss is   27542.975022629165\n",
      "loss is   27542.975022622377\n",
      "loss is   27542.975022616207\n",
      "loss is   27542.975022610615\n",
      "loss is   27542.975022605573\n",
      "loss is   27542.975022601\n",
      "loss is   27542.97502259685\n",
      "loss is   27542.97502259306\n",
      "loss is   27542.975022589617\n",
      "loss is   27542.97502258655\n",
      "loss is   27542.97502258372\n",
      "loss is   27542.975022581195\n",
      "loss is   27542.975022578874\n",
      "loss is   27542.975022576775\n",
      "loss is   27542.97502257493\n",
      "loss is   27542.975022573162\n",
      "loss is   27542.97502257163\n",
      "loss is   27542.975022570216\n",
      "loss is   27542.975022568906\n",
      "loss is   27542.975022567745\n",
      "loss is   27542.97502256672\n",
      "loss is   27542.975022565744\n",
      "loss is   27542.975022564904\n",
      "loss is   27542.97502256411\n",
      "loss is   27542.975022563398\n",
      "loss is   27542.97502256276\n",
      "loss is   27542.97502256222\n",
      "loss is   27542.97502256167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is   27542.975022561193\n",
      "loss is   27542.975022560757\n",
      "loss is   27542.975022560368\n",
      "loss is   27542.97502255996\n",
      "loss is   27542.975022559673\n",
      "loss is   27542.97502255939\n",
      "loss is   27542.975022559116\n",
      "loss is   27542.97502255887\n",
      "loss is   27542.97502255864\n",
      "loss is   27542.975022558458\n",
      "loss is   27542.975022558276\n",
      "loss is   27542.97502255811\n",
      "loss is   27542.975022557956\n",
      "loss is   27542.975022557835\n",
      "loss is   27542.975022557694\n",
      "loss is   27542.975022557603\n",
      "loss is   27542.975022557504\n",
      "loss is   27542.97502255741\n",
      "loss is   27542.975022557337\n",
      "loss is   27542.97502255725\n",
      "loss is   27542.975022557166\n",
      "loss is   27542.975022557108\n",
      "loss is   27542.97502255707\n",
      "loss is   27542.975022557006\n",
      "loss is   27542.975022556973\n",
      "loss is   27542.97502255694\n",
      "loss is   27542.97502255692\n",
      "loss is   27542.97502255687\n",
      "loss is   27542.975022556828\n",
      "loss is   27542.9750225568\n",
      "loss is   27542.975022556773\n",
      "loss is   27542.975022556748\n",
      "loss is   27542.97502255673\n",
      "loss is   27542.97502255671\n",
      "loss is   27542.97502255672\n",
      "loss is   27542.97502255667\n",
      "loss is   27542.97502255668\n",
      "loss is   27542.975022556668\n",
      "loss is   27542.975022556653\n",
      "loss is   27542.97502255665\n",
      "loss is   27542.97502255664\n",
      "loss is   27542.975022556624\n",
      "loss is   27542.975022556613\n",
      "loss is   27542.97502255661\n",
      "loss is   27542.97502255659\n",
      "loss is   27542.9750225566\n",
      "loss is   27542.97502255658\n",
      "loss is   27542.975022556573\n",
      "loss is   27542.97502255657\n",
      "loss is   27542.975022556577\n",
      "loss is   27542.975022556573\n",
      "loss is   27542.975022556562\n",
      "loss is   27542.97502255654\n",
      "loss is   27542.975022556577\n",
      "loss is   27542.97502255655\n",
      "loss is   27542.97502255655\n",
      "loss is   27542.975022556544\n",
      "loss is   27542.97502255657\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556588\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556573\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255655\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255654\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556573\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255657\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255655\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255648\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.97502255651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556482\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556537\n",
      "loss is   27542.97502255654\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556544\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556544\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556544\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255654\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556573\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255657\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556482\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556562\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255657\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556577\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255655\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.97502255648\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556548\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255656\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556544\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556486\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.975022556493\n",
      "loss is   27542.97502255655\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.97502255654\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556533\n",
      "loss is   27542.97502255653\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556562\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556515\n",
      "loss is   27542.975022556508\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255651\n",
      "loss is   27542.97502255652\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556555\n",
      "loss is   27542.9750225565\n",
      "loss is   27542.97502255649\n",
      "loss is   27542.975022556526\n",
      "loss is   27542.975022556497\n",
      "loss is   27542.975022556522\n",
      "loss is   27542.975022556504\n",
      "loss is   27542.975022556533\n",
      "Optimizer information:\n",
      "     fun: 2696.5004794367887\n",
      "     jac: array([-1.69078121, -0.61970854, -4.66049791, -6.93644843,  2.52231396,\n",
      "        2.40988205, -0.93656189, -2.50887235,  6.36132016, -1.61485767,\n",
      "       -0.3167582 ])\n",
      " message: 'Maximum number of iterations has been exceeded.'\n",
      "    nfev: 12\n",
      "     nit: 1\n",
      "    njev: 12\n",
      "  status: 1\n",
      " success: False\n",
      "       x: array([ 1.00477093,  0.21679617,  0.23574867,  0.77607738,  0.08017162,\n",
      "        1.24212583,  1.02006702,  0.58348632,  0.99761884,  0.47992048,\n",
      "       -3.36194694])\n",
      "Predicting outputs\n"
     ]
    }
   ],
   "source": [
    "accuracies=[]\n",
    "c_list=[100]\n",
    "for c in c_list:\n",
    "    print(\"Started training for c = \",c)\n",
    "    trained_model_parameters = train(train_data_loader, \"square_hinge_loss\", \"L2\", c)\n",
    "    print(\"Predicting outputs\")\n",
    "    test_data_output = test(test_data, trained_model_parameters)\n",
    "    accuracies.append(np.sum(actual_targets[:,1]==test_data_output)/300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9233333333333333]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "plt.plot(c_list,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Loss with L4 regularization: best accuracy at c=1 (0.937)\n",
    "## Square Hinge Loss with L2 regularization: best accuracy at c=100 (0.923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
