{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-bf53b74469b6>, line 148)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-5-bf53b74469b6>\"\u001b[1;36m, line \u001b[1;32m148\u001b[0m\n\u001b[1;33m    with open(data_file, 'r') as df:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader(object):\n",
    "    # this class has a standard iterator declared\n",
    "    # __len__ returns the number of batches (size of the object)\n",
    "    # __get_item__ handles integer based indexing of the object \n",
    "    def __init__(self, data_file, batch_size):import argparse\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import utils\n",
    "import numpy as np\n",
    "\n",
    "class DataLoader(object):\n",
    "    # this class has a standard iterator declared\n",
    "    # __len__ returns the number of batches (size of the object)\n",
    "    # __get_item__ handles integer based indexing of the object \n",
    "    def __init__(self, data_file, batch_size):\n",
    "        with open(data_file, 'r') as df:\n",
    "            data = df.readlines()\n",
    "\n",
    "        data = data[1:]\n",
    "        data = data[:(len(data)//batch_size)*batch_size]\n",
    "        np.random.shuffle(data)\n",
    "        data = np.array([[float(col) for col in row.split(',')] for row in data])\n",
    "        input_data, targets = data[:, :-1], data[:, -1]\n",
    "        input_data = np.hstack([input_data, np.ones((len(input_data), 1), dtype=np.float32)])\n",
    "        input_data = (input_data-input_data.mean(axis=0))/(input_data.max(axis=0)-input_data.min(axis=0))\n",
    "\n",
    "        self.num_features = input_data.shape[1]\n",
    "        self.current_batch_index = 0\n",
    "        self.input_batches = np.split(input_data, len(input_data)//batch_size)\n",
    "        self.target_batches = np.split(targets, len(targets)//batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_batches)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "\n",
    "        batch_input_data = self.input_batches[i]\n",
    "        batch_targets = self.target_batches[i]\n",
    "        return batch_input_data, batch_targets\n",
    "\n",
    "def classify(inputs, weights):\n",
    "    #this functions returns w^Tx . The output  is batch_size*1\n",
    "\treturn np.dot(inputs, np.reshape(weights, (np.size(weights), 1)).reshape((-1,)))\n",
    "\n",
    "def get_objective_function(trainx,trainy,loss_type, regularizer_type, loss_weight):\n",
    "    # this function calculates the loss for a current batch\n",
    "    loss_function = utils.loss_functions[loss_type]\n",
    "    if regularizer_type != None:\n",
    "\n",
    "        regularizer_function = utils.regularizer_functions[regularizer_type]\n",
    "    def objective_function(weights):\n",
    "        loss = 0\n",
    "        \n",
    "        inputs, targets = trainx,trainy\n",
    "        outputs = classify(inputs, weights)\n",
    "        loss += loss_weight*loss_function(targets, outputs)\n",
    "        if regularizer_type != None:\n",
    "            # regulariser function is called from utils.py\n",
    "            loss += regularizer_function(weights)\n",
    "        return loss\n",
    "    return objective_function\n",
    "\n",
    "def get_gradient_function(trainx,trainy,loss_type, regularizer_type, loss_weight):\n",
    "    # This is a way to declare function inside a function \n",
    "    # The get_gradient_function receives the train data from the current batch\n",
    "    # and all other parameters on which the loss function and gradient depend\n",
    "    # like C,regulariser_type and loss function\n",
    "    loss_grad_function = utils.loss_grad_functions[loss_type]\n",
    "    if regularizer_type != None:\n",
    "        regularizer_grad_function = utils.regularizer_grad_functions[regularizer_type]\n",
    "    # gradient function is called from scipy.optimise.minimise()\n",
    "    # the only paramter its can send is weights \n",
    "    # hence there was a need to pass the current batch through get_objective_function\n",
    "\n",
    "\n",
    "    def gradient_function(weights):\n",
    "\n",
    "        gradient = np.zeros(len(weights), dtype=np.float32)\n",
    "        X=trainx\n",
    "        Y=trainy\n",
    "        outputs = classify(X,weights)\n",
    "        # loss_grad_function is called from utils.py\n",
    "        gradient = loss_weight*loss_grad_function(weights,X,Y,outputs)/len(trainx)\n",
    "        if regularizer_type != None:\n",
    "            # regulariser grad function is called from utils.py\n",
    "            gradient += regularizer_grad_function(weights)\n",
    "        return gradient\n",
    "    return gradient_function\n",
    "\n",
    "def train(data_loader, loss_type, regularizer_type, loss_weight):\n",
    "    initial_model_parameters = np.zeros(data_loader.num_features)\n",
    "\n",
    "    num_epochs=500\n",
    "    for i in range(num_epochs):\n",
    "        loss=0\n",
    "        if(i==0):\n",
    "            start_parameters=initial_model_parameters\n",
    "        for j in range(len(data_loader)):\n",
    "            trainx,trainy=data_loader[j]\n",
    "            objective_function = get_objective_function(trainx,trainy,loss_type, \n",
    "                                                regularizer_type,loss_weight)\n",
    "            gradient_function = get_gradient_function(trainx,trainy, loss_type, \n",
    "                                              regularizer_type, loss_weight)\n",
    "            # to know about this function please read about scipy.optimise.minimise\n",
    "            trained_model_parameters = minimize(objective_function, \n",
    "                                        start_parameters, \n",
    "                                        method=\"CG\", \n",
    "                                        jac=gradient_function,\n",
    "                                        options={'disp': False,\n",
    "                                                 'maxiter': 1})\n",
    "            loss+=objective_function(trained_model_parameters.x)\n",
    "            start_parameters=trained_model_parameters.x\n",
    "        # prints the batch loss\n",
    "        print(\"loss is  \",loss)\n",
    "        \n",
    "    print(\"Optimizer information:\")\n",
    "    print(trained_model_parameters)\n",
    "    return trained_model_parameters.x\n",
    "            \n",
    "\n",
    "def test(inputs, weights):\n",
    "    outputs = classify(inputs, weights)\n",
    "    probs = 1/(1+np.exp(-outputs))\n",
    "    # this is done to get all terms in 0 or 1 You can change for -1 and 1\n",
    "    return np.round(probs)\n",
    "\n",
    "def write_csv_file(outputs, output_file):\n",
    "    # dumps the output file\n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        out_file.write(\"ID, Output\\n\")\n",
    "        for i in range(len(outputs)):\n",
    "            out_file.write(\"{}, {}\".format(i+1, str(outputs[i])) + \"\\n\")\n",
    "def get_data(data_file):\n",
    "    with open(data_file, 'r') as df:\n",
    "        data = df.readlines()\n",
    "\n",
    "    data = data[1:]\n",
    "    data = np.array([[float(col) for col in row.split(',')] for row in data])\n",
    "    input_data = np.hstack([data, np.ones((len(data), 1), dtype=np.float32)])\n",
    "\n",
    "    return input_data\n",
    "\n",
    "        with open(data_file, 'r') as df:\n",
    "            data = df.readlines()\n",
    "\n",
    "        data = data[1:]\n",
    "        data = data[:(len(data)//batch_size)*batch_size]\n",
    "        np.random.shuffle(data)\n",
    "        data = np.array([[float(col) for col in row.split(',')] for row in data])\n",
    "        input_data, targets = data[:, :-1], data[:, -1]\n",
    "        input_data = np.hstack([input_data, np.ones((len(input_data), 1), dtype=np.float32)])\n",
    "\n",
    "        self.num_features = input_data.shape[1]\n",
    "        self.current_batch_index = 0\n",
    "        self.input_batches = np.split(input_data, len(input_data)//batch_size)\n",
    "        self.target_batches = np.split(targets, len(targets)//batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_batches)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "\n",
    "        batch_input_data = self.input_batches[i]\n",
    "        batch_targets = self.target_batches[i]\n",
    "        return batch_input_data, batch_targets\n",
    "\n",
    "def classify(inputs, weights):\n",
    "    #this functions returns w^Tx . The output  is batch_size*1\n",
    "\treturn np.dot(inputs, np.reshape(weights, (np.size(weights), 1)).reshape((-1,)))\n",
    "\n",
    "def get_objective_function(trainx,trainy,loss_type, regularizer_type, loss_weight):\n",
    "    # this function calculates the loss for a current batch\n",
    "    loss_function = utils.loss_functions[loss_type]\n",
    "    if regularizer_type != None:\n",
    "\n",
    "        regularizer_function = utils.regularizer_functions[regularizer_type]\n",
    "    def objective_function(weights):\n",
    "        loss = 0\n",
    "        \n",
    "        inputs, targets = trainx,trainy\n",
    "        outputs = classify(inputs, weights)\n",
    "        loss += loss_weight*loss_function(targets, outputs)\n",
    "        if regularizer_type != None:\n",
    "            # regulariser function is called from utils.py\n",
    "            loss += regularizer_function(weights)\n",
    "        return loss\n",
    "    return objective_function\n",
    "\n",
    "def get_gradient_function(trainx,trainy,loss_type, regularizer_type, loss_weight):\n",
    "    # This is a way to declare function inside a function \n",
    "    # The get_gradient_function receives the train data from the current batch\n",
    "    # and all other parameters on which the loss function and gradient depend\n",
    "    # like C,regulariser_type and loss function\n",
    "    loss_grad_function = utils.loss_grad_functions[loss_type]\n",
    "    if regularizer_type != None:\n",
    "        regularizer_grad_function = utils.regularizer_grad_functions[regularizer_type]\n",
    "    # gradient function is called from scipy.optimise.minimise()\n",
    "    # the only paramter its can send is weights \n",
    "    # hence there was a need to pass the current batch through get_objective_function\n",
    "\n",
    "\n",
    "    def gradient_function(weights):\n",
    "\n",
    "        gradient = np.zeros(len(weights), dtype=np.float32)\n",
    "        X=trainx\n",
    "        Y=trainy\n",
    "        outputs = classify(X,weights)\n",
    "        # loss_grad_function is called from utils.py\n",
    "        gradient = loss_weight*loss_grad_function(weights,X,Y,outputs)/len(trainx)\n",
    "        if regularizer_type != None:\n",
    "            # regulariser grad function is called from utils.py\n",
    "            gradient += regularizer_grad_function(weights)\n",
    "        return gradient\n",
    "    return gradient_function\n",
    "\n",
    "def train(data_loader, loss_type, regularizer_type, loss_weight):\n",
    "    initial_model_parameters = np.random.random((data_loader[0][0].shape[1]))\n",
    "\n",
    "    num_epochs=1000\n",
    "    for i in range(num_epochs):\n",
    "        loss=0\n",
    "        if(i==0):\n",
    "            start_parameters=initial_model_parameters\n",
    "        for j in range(len(data_loader)):\n",
    "            trainx,trainy=data_loader[0][j], data_loader[1][j]\n",
    "            objective_function = get_objective_function(trainx,trainy,loss_type, \n",
    "                                                regularizer_type,loss_weight)\n",
    "            gradient_function = get_gradient_function(trainx,trainy, loss_type, \n",
    "                                              regularizer_type, loss_weight)\n",
    "            # to know about this function please read about scipy.optimise.minimise\n",
    "            trained_model_parameters = minimize(objective_function, \n",
    "                                        start_parameters, \n",
    "                                        method=\"CG\", \n",
    "                                        jac=gradient_function,\n",
    "                                        options={'disp': False,\n",
    "                                                 'maxiter': 1})\n",
    "            loss+=objective_function(trained_model_parameters.x)\n",
    "            start_parameters=trained_model_parameters.x\n",
    "        # prints the batch loss\n",
    "        print(\"loss is  \",loss)\n",
    "        \n",
    "    print(\"Optimizer information:\")\n",
    "    print(trained_model_parameters)\n",
    "    return trained_model_parameters.x\n",
    "            \n",
    "\n",
    "def test(inputs, weights):\n",
    "    outputs = classify(inputs, weights)\n",
    "    probs = 1/(1+np.exp(-outputs))\n",
    "    # this is done to get all terms in 0 or 1 You can change for -1 and 1\n",
    "    return np.round(probs)\n",
    "\n",
    "def write_csv_file(outputs, output_file):\n",
    "    # dumps the output file\n",
    "    with open(output_file, \"w\") as out_file:\n",
    "        out_file.write(\"ID, Output\\n\")\n",
    "        for i in range(len(outputs)):\n",
    "            out_file.write(\"{}, {}\".format(i+1, str(outputs[i])) + \"\\n\")\n",
    "def get_data(data_file):\n",
    "    with open(data_file, 'r') as df:\n",
    "        data = df.readlines()\n",
    "\n",
    "    data = data[1:]\n",
    "    data = np.array([[float(col) for col in row.split(',')] for row in data])\n",
    "    input_data = np.hstack([data, np.ones((len(data), 1), dtype=np.float32)])\n",
    "\n",
    "    return input_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got files\n"
     ]
    }
   ],
   "source": [
    "train_data_loader = DataLoader(\"../kaggle 2-a/train.csv\",70)\n",
    "test_data = get_data(\"../kaggle 2-a/test.csv\")\n",
    "print(\"Got files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "actual_targets=pd.read_csv(\"targets.csv\")\n",
    "actual_targets=actual_targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data_loader[:9]\n",
    "\n",
    "validation_data=train_data_loader[10:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training for c =  1\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "loss is   2147700.01707984\n",
      "Optimizer information:\n",
      "     fun: 953472.181421799\n",
      "     jac: array([ 9.61281745e+04,  4.09738537e+02,  4.96430679e+02,  1.51690359e+02,\n",
      "       -5.50085592e+01,  1.47478678e+02])\n",
      " message: 'Desired error not necessarily achieved due to precision loss.'\n",
      "    nfev: 18\n",
      "     nit: 0\n",
      "    njev: 14\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([0.27649816, 0.41242744, 0.39325959, 0.80796001, 0.12563374,\n",
      "       0.24042543])\n",
      "Predicting outputs\n",
      "26\n",
      "28\n",
      "24\n",
      "37\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "accuracies=[]\n",
    "c_list=[1]\n",
    "for c in c_list:\n",
    "    print(\"Started training for c = \",c)\n",
    "    trained_model_parameters = train(train_data, \"square_hinge_loss\", None, c)\n",
    "    print(\"Predicting outputs\")\n",
    "    accuracy=0\n",
    "    for i in range(len(validation_data[0])):\n",
    "        test_data_output = test(validation_data[0][i], trained_model_parameters)\n",
    "        accuracy=np.sum(validation_data[1][i]==test_data_output)\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[1][0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data_loader[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15714285714285714"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xscale('log')\n",
    "plt.plot(c_list,accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron Loss with L4 regularization: best accuracy at c=1 (0.937)\n",
    "## Square Hinge Loss with L2 regularization: best accuracy at c=100 (0.923)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.,\n",
       "       0., 1.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
